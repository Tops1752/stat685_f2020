---
title: "STAT 685: Dr. Suojin Wang's Group"
subtitle: |
  | Modeling Seoul Bike Sharing Demand
author: "Nam Tran, Bai Zou"
date: \today
header-includes:
   - \usepackage{fontspec}
   - \setmainfont{Times New Roman}
   - \usepackage{setspace}
   - \onehalfspacing
   - \usepackage{enumitem}
   - \usepackage{subfig}
   - \usepackage{url}
   - \usepackage{listings}
   - \usepackage{multirow}
   - \pagenumbering{gobble}
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    fig_caption: true
    includes:
      in_header: preamble.tex
documentclass: report
fontsize: 12pt
geometry: margin=1in 
urlcolor: blue
---

```{r packages, message=FALSE, echo=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = TRUE, fig.height = 3, out.extra = "")

require(tidyverse)
require(dplyr)
require(caret) 
require(glmnet)
require(doParallel)
require(earth)
require(vip)
require(ranger)
require(xgboost)
require(RANN)
require(lubridate)
require(flipTime)
require(e1071)
require(latex2exp)
require(aTSA)
require(forecast)

theme_update(plot.title = element_text(hjust = 0.5))
```

\newpage

\pagenumbering{roman}
\setcounter{page}{2}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage

\pagenumbering{arabic}

\chapter{Introduction}

```{r data_setup, cache=TRUE, message=FALSE, echo=FALSE}
set.seed(1)

# Loading the Data
datPath = paste0(dirname(getwd()), "/data/SeoulBikeData.csv")
colNames = c("Date", "RentedBikeCount", "Hour", "Temp", "Humidity", 
             "WindSpeed", "Visibility", "DewPointTemp", "SolarRadiation",
             "Rainfall", "Snowfall", "Seasons", "Holiday", "FunctionalDay")
dat = read_csv(datPath, col_names = colNames, skip=1)

# Setting up Factors
dat$Hour = as_factor(dat$Hour)
dat$Seasons = as_factor(dat$Seasons)
dat$Holiday = as_factor(dat$Holiday)
dat$FunctionalDay = as_factor(dat$FunctionalDay)

# Creating DateTime that incorporates both Date and Hours and dropping Hour
dat$Date = AsDateTime(dat$Date) + hours(dat$Hour)

# We consider our "train" dataset all that we have, i.e., the total, for which we'll split later. 
Xtot = dat %>% select(-RentedBikeCount, -Date)
Ytot = dat %>% pull(RentedBikeCount)

# Partitioning
anchorDate = "2018/11/01"
learnNdx = dat$Date < anchorDate

yl = Ytot[learnNdx]
yt = Ytot[!learnNdx]
xl = Xtot[learnNdx,]
xt = Xtot[!learnNdx, ]

# Qualtitative Features
featureLevels = sapply(xl, function(x) { length(unique(x)) })
#head(sort(featureLevels), 5)

# Missing Data
featureNA   = head(sort(sapply(xl, function(x) { sum(is.na(x) )}), decreasing = TRUE))
obsNA       = head(sort(apply(xl, 1, function(x) { sum(is.na(x) )}), decreasing = TRUE))
naSummaryDf = data.frame(featureNA=featureNA, obsNA=obsNA, row.names = 1:length(obsNA))

# Data Normalization
# ---------------------------------------
# Identifying qualitative features
# ---------------------------------------
qualFeatures = which(featureLevels < 30)

xlQuant = xl %>% select(-all_of(qualFeatures))
xlQual  = xl %>% select(all_of(qualFeatures))
xtQuant = xt %>% select(-all_of(qualFeatures))
xtQual  = xt %>% select(all_of(qualFeatures))

# ---------------------------------------
# Center/Scale of Quantitative Features
# ---------------------------------------
ppStandardization = preProcess(xlQuant, method=c("center", "scale"))
xlQuantPost = predict(ppStandardization, xlQuant)
xtQuantPost = predict(ppStandardization, xtQuant)

# ---------------------------------------
# Creating Dummary Variables 
# ---------------------------------------
ppDummy = dummyVars(~ ., data = xlQual, fullRank = TRUE)

xlQualPost = predict(ppDummy, xlQual)
xtQualPost = predict(ppDummy, xtQual)

# ---------------------------------------
# Getting a singular matrix now
# ---------------------------------------
xlFull = cbind(xlQuantPost, xlQualPost)
xtFull = cbind(xtQuantPost, xtQualPost)

xlFullMat = as.matrix(xlFull)
xtFullMat = as.matrix(xtFull)

# Correlation Amongst the Features
corMat = cor(xlFull)
absCorMat = abs(corMat)[upper.tri(corMat)]
#round(quantile(absCorMat, c(0.5, 0.75, 0.9, 0.95, 0.99, 1)), 3)

# Identifying outliers
pcaOut = prcomp(xlFull, center=FALSE, scale=FALSE)
#pcaOut$x[,1:2] %>% as.data.frame %>% ggplot() +
#  geom_point(aes(x=PC1, y=PC2))

# Setting up multi-cluster
cl = makeCluster(6)
registerDoParallel(cl)

trControl = trainControl(method = "cv", number = 10)
```

\section{Background}

Our data set is the "Seoul Bike Sharing Demand Data Set", which on a high level contains hourly data for bike usage as well as various covariates that might be useful, e.g., temperature. Further, it contains around one year of data.

The data set has been aggregated and been uploaded to the UCI Machine Learning Repository, located here: http://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand

At first glance, relevant pieces are:

* Contains 8760 observations

* There are 14 columns

Regarding motivation for the data set and its potential use, the following is taken from the UCI website and was attached by the team that donated the data: "

"Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.

The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information."

\section{Previous Work}

There are two papers dealing with this data set explicitly. We're including both papers and in addition providing high level commentary on the paper and our concerns. 

\subsection{Paper 1: VE, Park, and Cho (2020, Computer Communications), Using Data Mining Techniques}

**High Level:**

* Attempted to predict the hourly demand.

* Claims .96 R^2 and .92 R^2 for train and test, respectively. 

* Uses “Boruta” to identify important features, which underneath the hood uses random forest as part of the algorithm. 

* Considered linear regression, GBM, SVM’s (radial basis function), boosted trees, and xgboost; had a nice mathematical summary of each. 

**Thoughts/Concerns**

* Didn't consider L1 Regularization, i.e., LASSO, which seems the most obvious for feature importance/selection/ranking. For example, do the full path as we go over $\lambda$ and see the order they get dropped, where the sooner they drop the less important they are.

* Make no reference to how they split up into 75% train and 25% test, e.g., is it interleave of a specific calendar day and everything after is post? If interleaved, the the 75% train’s distribution and 25% test’s distribution are effectively identical and learning on the train portion is considered “cheating” since data has leaked. 

* Didn't consider non-linear transforms of the data, which may not be that important given the usage of decision trees, but could have allowed plain linear regression to perform better.


\subsection{Paper 2: VE and Cho (2020, European Journal of Remote Sensing), A Rule-Based Model}

**High Level**

* Consider CUBIST, regularized random forest, CART, KNN, Conditional Inference Tree.

* Almost identical to previous paper. 

* They had an additional data source, the “Capital Bikeshare program,” for which the dataset came from Kaggle. It didn’t seem that they used this in an “interesting” way (interleaving the data, using it as a validation) but instead just considered it as another data source for which they ran their same methodology and if they got the same R^2 and other metrics, then they'd consider that would be successful. 

**Thoughts/Concerns**

* Still don’t talk about train/test split methodology. 

* The last bullet point of high level thoughts. 


\section{Scope and Goal}

Based on the description above, we break it into two potential business requirements here:

* Predict next day hourly demand based on historical data until the current day.

* Real-time prediction for next hour demand based on historical data until the current hour.

The scope in this study is to:

* Re-define training and testing data with anchor time.

* Re-evaluate estimation methods with data splitting by anchor time.

* Build forecasting models to predict the next hour demand and compare the models in terms of prediction accuracy, prediction variance, running time, etc.


\newpage
\chapter{Data Exploratory}

\section{Seoul Bike Sharing Demand Data}
* Downloaded the data from the [UCI Machine Learning Repo](https:/http://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand).
* Contains 8760 measurements of number of bikes rented over 364.958 days.

Features of the data are,

* `DateTime`
* `RentedBikeCount`
* `Temp`, in Celsius.
* `Humidity`, in percent, max of 100.
* `Windspeed`
* `Visibility` out to 10 meters.
* `DewPointTemp`, in Celsius.
* `SolarRadiation`
* `Rainfall`, in mm.
* `Snowfall`, in cm.
* `Seasons`, a factor with levels {Winter, Spring, Summer, Autumn}.
* `Holiday`, a factor with levels {Holiday, No holiday}.
* `FunctionalDay`, a factor with levels {NoFunc(Non Functional Hours), Fun(Functional hours)}


\section{Time Series Data}

Fundamentally, our data is time-series data (Figure 2.1). As such, let $y_t$ be the time series we're working to model, i.e., Seoul's bike sharing data. 

```{r yt_data_plot, cache = TRUE, dependson = 'data_setup', fig.cap="Hourly Rented Bike Count Over Entire Time Period", echo=FALSE}
dat %>% 
  ggplot(aes(x=Date, y=RentedBikeCount)) +
  geom_line() +
  labs(y="Rented Biked Count", "Date Time (Hour)")
```

\subsection{Stationarity}

It's arguable that there might be a strong seasonality component (less in winter more in summer), but that's hard to ascertain here since we only have one year's of data and only have one cycle. Further, there might be strong seasonality on an intraday basis (less in early morning and ramp up afterwards). If there was a strong seasonality component, we'd say our data isn't stationary, since on a first order basis, $E(y_t)$ will be dependent on $t$. Stationarity is important for a multitude of reasons, including *averaging being meaningful* and any *conditional expectation model we build is stable*.

Note, we can still incorporate terms to make a time series stationary, e.g., trend-stationary. 

We can test this directly using the Augmented Dickey-Fuller (ADF) Test, which intuitively tests for the presence of a unit root, which implies non-stationarity. $H_0$ for ADF is that $y_t$ is non-stationary, and $H_a$ is that $y_t$ is stationary. Note there are different types of stationarity, e.g., in presence of drift ($\mu$) or linear trend ($\beta t$). 

```{r yt_adf_plot, cache = TRUE, dependson = 'data_setup', fig.cap="Augmented Dickey Fuller (ADF) Test for Stationarity", echo=FALSE}
zz = adf.test(dat$RentedBikeCount, 100, output=FALSE)
adf1 = zz[[1]] %>% data.frame %>% mutate(Type="No Drift, No Trend")
adf2 = zz[[2]] %>% data.frame %>% mutate(Type="With Drift, No Trend")
adf3 = zz[[3]] %>% data.frame %>% mutate(Type="With Drift, With Trend")
adfDat = bind_rows(adf1, adf2, adf3)

adfDat %>% ggplot(aes(x=lag, y=p.value, color=Type)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept=0.05, linetype="dashed", color = "black") +
  labs(x="Lag", y="p-value")
```

Note that each lag, i.e., tick mark, in the ADF figure represents an hour. Under the most relaxed condition of no drift and no trend, then we can see that we start getting significant non-stationarity post lag 48, which represents approximately two days past. While this can be handled by differencing, as suggested by the stationarity for more restrictive conditions, this can also suggest that we can include lagged covariates of the response, i.e., lagged $y_t$, which we will ascertain next when looking at the auto-correlation function (ACF) plots and the partial auto-correlation function (PACF) plots. 


\subsection{Autocorrelation and Partial Autocorrelation of Rented Biked Count, $y_t$}

The ACF looks at correlation of $y_t$ with lagged versions of itself, e.g., $y_{t-k}$. The PACF differs in that it looks at correlation of $y_t$ with lagged versions of itself, e.g., $y_{t-k}$, while controlling for the intermediary lags, e.g., $\tilde{\bs{y}} = \{y_{t-1}, y_{t-2}, \dots, y_{t-k+1}\}$. From a practical standpoint, when considering PACF, we regress $y_t$ on $\tilde{\bs{y}}$ and $y_{t-k}$ on $\tilde{\bs{y}}$, and then look at the correlation of their respective residuals. 

Here, We look at the ACF and PACF of $y_t$ up to 100 and 50 lags.

```{r yt_acf_pacf, cache = TRUE, dependson = 'data_setup', fig.cap="Autocorrelation and Partial Autocorrelation of Rented Bike Count, i.e., $y_t$", echo=FALSE}
selfACF   = ggAcf(dat$RentedBikeCount, lag.max = 100) + labs(title=TeX("$y_t$"))
selfPACF  = ggPacf(dat$RentedBikeCount, lag.max = 100) + labs(title=TeX("$y_t$"))
selfACF2  = ggAcf(dat$RentedBikeCount, lag.max = 50) + labs(title=TeX("$y_t$"))
selfPACF2 = ggPacf(dat$RentedBikeCount, lag.max = 50) + labs(title=TeX("$y_t$"))

plots = list(selfACF, selfPACF, selfACF2, selfPACF2)
gridExtra::grid.arrange(grobs = plots, ncol=2)
```



Math theory states that an AR($p$) model would have a hard cutoff to zero in the PACF plot for $h > p$, and a MA($q$) model would have a hard cutoff to zero in the ACF plot for $h > q$. From the ACF plot and seeing statistically significant autocorrelations all the way out, a simple MA($q$) model will not suffice. Looking at the PACF plot, we see a strong "cut-off" at around lag 25, suggesting an AR($25$) model. Needless to say, an AR($25$) model isn't very palatable and doesn't seem parsimonious. As such, we seemingly can't get away with a simple MA($q$) nor a simple AR($p$) model.

While we can't get a simple AR($p) or MA($q$) model, we can still use the results of the ACF and PACF plots to suggest that we need lagged values of our supervisor as additional covariates.


\section{Feature Attributes}


\subsection{Hourly Trend}

Plot below is showing the mean hourly demand by season. It is clear that winter season has much lower demand and summer season has relatively higher demand. Hourly trend is similar in each season with two peak time per day - 8 AM and 6 PM. The hour information could be used as either qualitative or quantitative since demand is not linearly related to hour. 

```{r yt_hourly_by_seasons, cache = TRUE, dependson = 'data_setup', fig.cap="Rented Bike Count by Hour Grouped by Seasons", echo=FALSE}
# plotting "effect" of seasons
dat %>% 
  mutate(Hour = hour(Date)) %>%
  group_by(Seasons, Hour) %>%
  summarise(MeanRentedBikeCount = mean(RentedBikeCount), .groups="drop") %>%
  ggplot(aes(x=Hour, y=MeanRentedBikeCount, color=Seasons)) +
  geom_line() +
  labs(y="Mean Rented Bike Count", x="Hour")  
```





\subsection{Qualitative Variables}

* The plots shows more rented bike count in non-holidays than holidays except for summer (Figure 2.5).

* If functional day is "no", there's no any bike rented (Figure 2.6).  

* Day of week is not making significant difference in rented bike count (Figure 2.7).

```{r yt_by_seasons_by_holiday, cache = TRUE, dependson = 'data_setup', fig.cap="Rented Bike Count by Season Grouped by Holiday", echo=FALSE}
dat %>% 
  ggplot(aes(x=Seasons, y=RentedBikeCount, fill=Holiday)) + 
  geom_boxplot() +
  scale_fill_brewer(palette="Paired") +
  labs(y="Rented Bike Count", x="Seasons") 
```

```{r yt_by_seasons_by_functionalday, cache = TRUE, dependson = 'data_setup', fig.cap="Rented Bike Count by Season Grouped by Functional Day", echo=FALSE}
dat %>% 
  ggplot(aes(x=Seasons, y=RentedBikeCount, fill=FunctionalDay)) + 
  geom_boxplot() +
  scale_fill_brewer(palette="Paired") +
  labs(y="Rented Bike Count", x="Seasons")
```


```{r yt_by_dayofweek_by_seasonsy, cache = TRUE, dependson = 'data_setup', fig.cap="Rented Bike Count by Day of Week Grouped by Season", echo=FALSE}
dat$DayOfWeek <- weekdays(dat$Date)

dat %>% 
  ggplot(aes(x=DayOfWeek, y=RentedBikeCount, fill=Seasons)) + 
  geom_boxplot() +
  scale_fill_brewer(palette="Paired") +
  labs(y="Rented Bike Count", x="DayOfWeek")
```


\subsection{Quantitative Variables}

Figure 2.8 and Figure 2.9 are showing correlations between quantitative variables and demand:

* The covariance matrix shows Temp, Hour has relatively higher correlation with RentedBikeCount (>0.4).

* DewPointTemp and SolarRadiation have correlation greater than 0.2. 

* Temp and DewPointTemp are highly correlated (0.9). 

* **No clear linear relationship can be identified between response variable and quantitative Variables**

![Caption for the picture.](./fig/2_cor_matrix.png)
*Figure 2.8 Variable Correlation Matrix*


![Caption for the picture.](./fig/2_cor_matrix_scatter.png)
*Figure 2.9 Top 4 Variable Correlation Matrix*


\section{Splitting Training and Testing Data}

The data set includes one year hourly bike rented count from Dec 2017 to Nov 2019. Splitting training and testing data in any anchor date will cause incomplete yearly distribution and information loss in training data. For example, there are only two days' observations for non-functional day before September 2018, which leaves little evidence for the model to identify the impact of Functional Day during training process if setting anchor date prior to September.

To minimize the information loss and maximize the training data size available, the testing anchor date and time will be set no earlier than November 1, 2018:

* Feature distributions are close to all year distribution (See 2.4.1 and 2.4.2).

* Preliminary model testing shows the best result when using Nov 2018 data for testing (Chapter 3).

\subsection{Weather Information Distribution}

Figure 2.10 and 2.11 below are comparing distributions of some weather features for all observations and subset of observations before September 1, October 1 and November 1, 2018. In general, the last subset (setting anchor day at November 1, 2018) has a close enough distribution comparing the one year data set.

![Caption for the picture 1.](./fig/2_feature_dist_1.png)
*Figure 2.10 Temp, Humidity and WindSpeed Distribution*


![Caption for the picture 2.](./fig/2_feature_dist_2.png)
*Figure 2.11 Visibility and DewPointTemp Distribution*


\subsection{Function Day and Holiday Distribution}

Tables below are showing number of observations by category in each data set. The last data set (setting anchor day at November 1, 2018) has the closest percentage comparing to the one year data. 

*Table 2.1 Number of Observations by Season*

![](./fig/2_table_season.png)

*Table 2.2 Number of Observations by Holidays*

![](./fig/2_table_holiday.png)


*Table 2.3 Number of Observations by Function Day*

![](./fig/2_table_function.png)


\newpage

\chapter{Estimation Methods Comparison}

\section{Theoretical Methodology}

\subsection{Data}
We have a collection of observations housed in a matrix $\bs{X}$ that is $\rdim{n}{p}$, i.e., $n$ observations and $p$ covariates. Further, for each observation, we have an associated supervisor value, for which all the supervisor values are housed in a column vector $\bs{y}$ that is $\rdim{n}{1}$, i.e., $n$ supervisor values for each of the $n$ associated observation. 

\subsection{Risk}
We want to use training data and different algorithms to produce a $\hat{f}: \R^p \mapsto \R$, such that we can make predictions with $\hat{f}$, i.e., $\hat{f}(\bs{X}) = \hat{y}$, where $\bs{x} \in \R^p$ and $\hat{y} \in \R$, such that $\hat{y}$ is a \say{good} prediction of $y$, the unobserved supervisor. 

One way to define \say{good} is to define it in the context of \say{loss}, specifically $\ell (\hat{y}, y)$. There are a multitude of loss functions to consider, but a \say{popular} loss for regression is the squared error loss, i.e., $\ell (\hat{y}, y) = (\hat{y} - y)^2$, where deviations from the true $y$ value is penalized in a squared fashion. 

We define \say{good} to be the risk for $f$, namely $R(f) = \E \ell (f(\bs{X}), \bs{Y})$, noting that $\bs{X}, \bs{Y}$ are random variables \textbf{but} $R(f)$ isn't random, due to the expectation. In practice, we can use \say{test error} as an estimate for the risk. We can also use \say{cross-validation} as another estimate for the risk as well. 

\subsection{Identifying $f_*$ vs. $\hat{f}$}
Let $f_* = \argminA_{f} R(f)$, i.e., $f_*$ has the lowest risk amongst the entire family of possible $f$. But, $f_*$ is theoretical, since we don't know the entire joint distribution of $(\bs{X}, \bs{Y})$. As such, $\hat{f}$ is our best guess of $f_*$. 

\subsection{Summary}
In short, our goal is to consider different algorithms and make the best predictions we can, as defined by jointly by our risk estimate and the embedded loss metric, squared error loss $\ell (\hat{y}, y) = (\hat{y} - y)^2$ in our case. We will estimate risk in two different ways, cross-validation to help guide our hyper-parameter selection, and \say{test error} as the final hold-out to evaluate the \say{tuned} hyper-parameters. 


\section{Linear Methods}

\subsection{Linear Regression}

Let $\bs{\beta}^T, \bs{x}^T \in \R^p$, then we wish to model $y$ as $y = \bs{x}^T \bs{\beta} + \epsilon$, i.e., we want to project $y$ onto the subspace spanned by $\bs{X}$. In short, $\hat{\beta} = \argminA_\beta ||\bs{y} - \bs{X} \bs{\beta}||^2_2$.

```{r fitLinearRegression, cache = TRUE, dependson = 'data_setup', echo=FALSE}
set.seed(1)
linearOut = train(x = xlFullMat, y = yl,
                  method = "lm",
                  trControl = trControl)

yhatLinear = predict(linearOut$finalModel, xlFull) %>%
  as.numeric()
```

```{r fitLinearRegression_residual_plot, fig.cap="Residual Plot for Linear Regression", cache=TRUE, dependson='fitLinearRegression', echo=FALSE, fig.height=3}
data.frame(e=yl - yhatLinear, yhat = yl) %>%
  ggplot(aes(x=yhat, y=e)) +
  geom_point() +
  labs(x=TeX("$\\hat{y}$"), y=TeX("$\\hat{\\epsilon} = y - \\hat{y}$"))
```

From this residual plot, it suggests that multiple linear regression isn't appropriate for this data set. Namely, in an idealized setting, we wouldn't notice any distinct patterns in the residuals, but in this case, we see a clear increase in residual values as our estimate $\hat{y}$ gets larger. This may be due to the supervisor being count data, for which a Poisson regression or applying a square-root transform to the supervisor. 

To be more specific, for us to do inference using Linear Regression, independent of prediction, we need to have $\E{\epsilon} = 0$, $\V(\epsilon)$, and $\text{Cov}(\epsilon_i, \epsilon_j) = 0$, for which all three conditions aren't satisfied. This isn't a problem specifically for us, since we care about prediction, but it does suggest model misspecification. 


\subsection{Elastic Net}

Elastic Net is an extension of Linear Regression, where we do a mixture of both of both $L_1$ regularization (penalty of $||\bs{\beta}||_1$) and $L_2$ regularization (penalty of $||\bs{\beta}||^2_2$). Then,  $\hat{\beta}(\lambda_1, \lambda_2) = \argminA_\beta ||\bs{y} - \bs{X} \bs{\beta}||^2_2 + \lambda_1 ||\bs{\beta}||_1 + \lambda_2 ||\bs{\beta}||_2^2$. 

Note most parameterizations of Elastic Net instead of having separate $\lambda_1$ and $\lambda_2$ have a singular $\lambda$ and a "mixing ratio" between $L_1$ and $L_2$ regularization in the form of $\alpha$. Then, $\hat{\beta}(\lambda, \alpha) = \argminA_\beta \big( ||\bs{y} - \bs{X}\bs{\beta}||^2_2 + \lambda ((1-\alpha)||\bs{\beta}||^2_2 + \alpha ||\bs{\beta}||_1) \big)$. In this parameterization, note that $\alpha=1$ results in LASSO, which has only $L_1$ regularization , and that $\alpha=0$ results in Ridge Regression, which has only $\L_2$ regularization. 

An open question remains though on how to choose $\alpha$, the mixture between $L_1$ and $L_2$ regularization, and $\lambda$, how much penalty to impose. For this, we can use $k$-fold cross-validation risk estimates. Namely, for a particular set of hyper-parameters we wish to consider, we can take our training data and split it into $k$ chunks, and for each chunk, we hold it out as the "test" data and learn on the remaining $k-1$ chunks (using our chosen hyper-parameters) and then evaluate on the $k^{\text{th}}$ holdout using, for example, squared error, i.e., $\sumi (y_i) - \hat{y}_i)^2$. We would then average over all $k$ risk estimatess and come up with a singular risk estimate, i.e., the cross-validation risk estimate for the hyper-parameters we used. Then, for the entire set of hyper-parameters, we'd have an associated cross-validation risk-estimate and consequently would choose the one with the lowest risk-estimate, which we're trying to minimmize.

```{r fitElasticNet, cache = TRUE, dependson = 'data_setup', fig.cap="Cross-Validation Risk Estimates for Elastic Net Hyper Parameters", echo=FALSE}
set.seed(1)
lassoGrid = expand.grid(
  lambda = seq(0, 1.25, length.out = 20),
  alpha = c(0.01, 0.05, 0.25, 0.5, 0.75, 1))
elasticOut = train(x = xlFullMat, y = yl,
                   method="glmnet",
                   tuneGrid = lassoGrid,
                   trControl = trControl)
plot(elasticOut)
```

Looking at the cross-validation risk estimates, the minimal test error is at $\hat{\alpha} = `r elasticOut$bestTune$alpha`$ and $\hat{\lambda} = `r elasticOut$bestTune$lambda`$, suggesting that we prefer minimal $L_1$ regularization ($\alpha = 0$ is strictly $L_2$ reguarlization). 

Note we've run into a boundary condition, i.e., we don't know if having $\hat{lambda} > `r elasticOut$bestTune$lambda`$ will result in an even lower risk estimate. As such, we expand past the boundary and see if we can get a lower risk estimate. 

```{r fitElasticNet2, cache = TRUE, dependson = 'data_setup', fig.cap="Cross-Validation Risk Estimates for Elastic Net Hyper Parameters Past Boundary Condition", echo=FALSE}
set.seed(1)
lassoGrid = expand.grid(
  lambda = seq(0.75, 5, length.out = 20),
  alpha = c(0.01))
elasticOut = train(x = xlFullMat, y = yl,
                   method="glmnet",
                   tuneGrid = lassoGrid,
                   trControl = trControl)
plot(elasticOut)
```

Thus, the hyper-parameters that minimize the cross-validation risk estimate is  $\hat{\alpha} = `r elasticOut$bestTune$alpha`$ and $\hat{\lambda} = `r elasticOut$bestTune$lambda`$.


```{r fitElasticNet2_yhat, cache=TRUE, dependson='fitElasticNet2', echo=FALSE}
# Refitting with 'core' glmnet
glmnetOut     = glmnet(x = xlFullMat, y = yl, alpha = elasticOut$bestTune$alpha)
betaHatGlmnet = coef(glmnetOut, s = elasticOut$bestTune$lambda)
yhatGlmnet    = predict(glmnetOut, xlFullMat, s=elasticOut$bestTune$lambda) %>%
  as.numeric()
```

```{r fitElasticNet2_residual_plot, cache=TRUE, dependson='fitElasticNet2', echo=FALSE, fig.height=3, fig.cap="Residual Plot for Elastic Net"}
data.frame(e = yl-yhatGlmnet, yhat = yl) %>%
  ggplot(aes(x=yhat, y=e)) +
  geom_point() +
  labs(x=TeX("$\\hat{y}$"), y=TeX("$\\hat{\\epsilon} = y - \\hat{y}$"))
```

Not surprisingly, the residual plot for Elastic Net shows similar behavior to Linear Regression, suggesting that Elastic Net isn't an appropriate model and further confirmation the linear models aren't appropriate for the problem we have. 

\subsection{Refitted Lasso}

\subsubsection{Theory}
Note lasso does both model selection as well as implicitly does regularization, which can produce a solution that has too much bias. To overcome this but still attain the benefits of model selection, we can choose a $\lambda_{1SE}$, i.e., the "one-standard-error" $\lambda$ and then refit with unreguarlized linear regression.

\subsubsection{Example}
```{r fitRefittedLasso, cache = TRUE, dependson = 'data_setup'}
set.seed(1)

# Fitting with glmnet in general with alpha = 1.0
glmnetOut     = cv.glmnet(x = xlFullMat, y = yl, alpha = 1.0)

# We use lambda.min since lambda.1se will result in an empty active set!
betaHatGlmnet = coef(glmnetOut, s = "lambda.min")
refittedSIdx = which(as.vector(abs(betaHatGlmnet) > 1e-6)[-1])

# Fitting an unregularized linear model
refittedLassoOut = train(x = xlFullMat[,refittedSIdx], y = yl,
                         method = "lm",
                         trControl = trControl)
```

```{r fitRefittedLasso_yhat, cache = TRUE, dependson = 'fitRefittedLasso'}
yhatRefitted = predict(refittedLassoOut, xlFullMat[,refittedSIdx])
```

\section{Non-Linear Methods}

\subsection{Multivariate Adaptive Regression Splines (MARS)}





\subsection{Decision Tree}

\subsection{Random Forest}

\subsection{Boosting}
\subsubsection{Theory}
Fitting, in a stepwise fashion, a greedy nonparametric procedure. Meta-parameters are $\lambda$, the learning rate, and $B$ the number of steps. 

\subsubsection{Example}

\newpage
\chapter{Forecasting Model Comparison}

There are three models considered to predict the next hour demand.(Assume the current day is X, and current hour is Y). 

* Model 1: One-time prediction for day X hourly demand by end of day X-1 with data updated to day X-1. 

* Model 2: Real-time model training to predict next hour (Y+1) demand with data updated to day X hour Y.

* Model 3: Continuous model training based on Model 1 and updated data from hour 1 to Y on day X. 


Testing data setting:

* Randomly select n(5 - 10 depends on run time) days as anchor days from Nov 2018:

  * For each anchor day, all observations prior to this anchor day is considered as training data.

  * For each anchor day, the next 24 hour observations is considered as testing data for Model 1.

  * There are n sets of training and testing data sets.

* Each anchor day selected above will have 24 sets of training and testing data based on anchor hour (Model 2 & Model 3).

* Or randomly select m anchor hours for each anchor day selected above (if run time becomes a consideration).


\section{Model 1: One-time Prediction}


\section{Model 2: Real-time Model Training}


\section{Model 3: Continuous Model Training}


\section{Model Comparison}


\newpage
\chapter{Result and Conclusion}





